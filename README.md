This blog is mainly posted to record DFGC2022 creation track participation experience and insights obtained by running different face swapping forgery methods. Details of coding are ignored.
In each section, organized by the sequence of conetest stage, the following content will be introduced:  
1. Analysis of the result in each stage.
2. Methods and strategies used in each stage
3. Trials and attempts been made in each stage of DFGC22 creation track  


## Takeaways of DFGC21
Before diving into the contest, It's helpful and efficient to learn from the past experience. Thus, I first conclude following key points from [DFGC2021 summary paper](https://arxiv.org/abs/2106.01217).  
(1) Top-3 methods in creation track use **adversarial training** to evade detection while all Top-3 methods of detection track train their models to defense against it. However, the defense works in a limited scenario in which training and testing datasets are similar and the adversarial noise generation methods used in training and testing are similar. Therefore, creation model can benefit more from adversarial training under the setting of DFGC22 where detection models are not allowed to train on the given dataset.  
(2) **Every step in the fogery pipeline should be treated seriously** to achieve the highest generation result. For example, the creation method, which ranks 3rd place in DFGC21, merely put great emphasis on the adversarial training and failed to achieve the best result in the competition.  
(3) **Finetuning existing fogery model** seems to be better than training it from scratch under the scenario of competition. The 2nd creation method trained a student model with pre-trained FaceShifter which requires extensive computational resources and time on tuning hyperparameters.  
(4) The **contexts** of Celeb-DF2 and videos provided in DFGC22 are different. For example, there are no obstacles on the face of image from Celeb-DF2 dataset, thus the blending method of creation track cannot be used directly.  
(5) Top-3 detetion methods distinguish fake images from real images based on **image domain**. Though methods based on frequency might be adopted by other teams, they did not achieve competitive scores. This might indicate the limitation of frequency based method.  
(6) Remarkable detection performance can be achieved when the **forgery information** is inferred or probed through multiple tries. This implies the importance of acquiring side information in the adversarial setting and leads to more effective strategies than blindly designing complicated models. 

# Stage1
## Methodology
- Extract every frame from all videos and save those frames in lossless format (png)
- Randomly pick one image for each video as the identity reference image guding face swapping
- **Start face swapping with official [SimSwap Code](https://github.com/neuralchen/SimSwap)**

## Observations
**[About generation]**  
(1) SimSwap model, trained on Celeb-HQ 512x512 images, could yield high resolution videos but is locally unstable. Specifically, the generated video did not preserve the temporal consistency within adjacent frames, which results in continuous "shaking" or "scaling" of sense organs in video  
![](shaking_face.mp4 "Figure: Shaking artifact")
(2) Simswap model trained on 224x224 images tend to generate low resolution swapped face (lower than real images of the same size).  
**[About blending]**  
(3) Edge artifact brought by the blending process is severe.  
![](blending_artifacts.png "Figure: Blending artifacts")

(4) Simswap cannot properly handle the situation in which there are **obstacles** on the face (e.g., fingers, etc.). see results/14/1/1  
![](obstacle.png "Figure: Failure case where obstacles exist")

**[minor]**   
(5) Artifacts brought by the built-in deficiencies (shown in figure ) of GAN (caused by upsampling) are obvious in several generated videos.  
![](gan_generated_artifacts.png "Figure: Common artifacts of GAN generated images")

(6) Features that requires high frequency information (e.g., whelk or spots on face) are not reproduced very well.
![](lack_high_freqinfo.png "Figure: Exsample in which SimSwap fails to reproduce high frequency information")


## Possible directions for improvement
Improvements can be made in three aspects:  
(1) For generation  
- Finetuning generator on every pair of source and target video improve quality and resolution and reduce noticable artifacts.
- Utilize diverse generation methods, e.g., Face2Face, to cover the algorithm specific artifacts.

(2) Improve the blending algorithm and consider the existance of obstacle
Finetuning videos (e.g. have obstacles on face) with more precise [blending algorithm](https://github.com/rotemtzaban/STIT) 

(3) Consider super-resolution method[3] to improve generated face image quality


## Submission
Specified 40 Videos generated by original SimSwap which was officially pretrained on 224x224 images.  



# Stage2
## Score of stage1  

| Subjective scores (graded by human)  | realisim | mouth | quality | expression |  ID  |
| ------------------------------------ | -------- | ----- | ------- | ---------- | ---- |
| ------------------------------------ |  2.675   |	3.91  |  2.945  |     3.4    | 2.78 |

| Anti-detecion score | Overall scores | Rank |
| ---- | ----   | ---- |
|2.367 | 18.077 | 3rd  |

> Compared with the method ranks No.1 on leaderboard, the **realisim** score( higher if more deceiving ), **image quality** score(higher if resolution matches with original 1920x1080 video) and **anti-detection** score should be paid much attention to.  
> Besides, as mentioned in the SimSwap paper and shown above, SimSwap can hardly perserve accurate identity information of source image in swapped image. This might caused by the fact that face identity extraction network used in training only support 112x112 face image and can be further improved.  

## Methodology
Since the source videos are processed before (frame extraction, face extraction), only face swapping method will be discussed here.  
(1) Face detection in SimSwap only used the face of which confidence score is maximum among all the predictions of insightface model. Thus, to stabilize the detection process of high-resolution face swapping (SimSwap 512 version) as suggested in [HRFS](https://studios.disneyresearch.com/2020/06/29/high-resolution-neural-face-swapping-for-visual-effects/), landmark stabilization is implemented by using customized [RetinaFace detection model](https://github.com/serengil/retinaface#:~:text=RetinaFace%20is%20a%20deep%20learning,is%20mainly%20based%20on%20mxnet.). The landmark predictions of RetinaFace detection model are simply averaged.  
(2) Swapped face with SimSwap is not high-resolution enough. Thus newly published version of SimSwap trained on 512x512 images is used.  
(3) Face blending in SimSwap is not good enough and method (Multi-band [splining](https://ai.stanford.edu/~kosecka/burt-adelson-spline83.pdf) and [GCF](https://diglib.eg.org/handle/10.2312/COMPAESTH.COMPAESTH05.159-167#:~:text=The%20newly%20introduced%20Global%20Contrast,factors%20needed%20to%20calculate%20GCF.) rectification) suggested in [HRFS](https://studios.disneyresearch.com/2020/06/29/high-resolution-neural-face-swapping-for-visual-effects/) is adopted. Since computing GCF takes a large amount of time, only the first swapped image and the target image is used to compute GCF respectively and the ratio (GCF(target)/GCF(swapped)) is used as constant to finetune swapped image.

```python
# multi-band splining pseudo-code
def HRFSspline(spath, dpath, mask, level=6):

    # Step 1:
    # get laplacian pyramid of source and target images
    # and get the guassian pyramid of target face mask
    source_lapyr = laplacian_pyramid(spath, level)
    dest_lapyr = laplacian_pyramid(dpath, level)
    mask_lapyr = gaussian_pyramid(mask, level)
    
    # Step 2: Blending two laplacian pyramids
    lapyr = []
    for i in range(level):
        lapyr.append((1-mask_lapyr[level-i-1])*dest_lapyr[i]) # extract non-face area in target image
        if i <= 2:
            # Use the lowest two level laplacian pyramid of target image
            # This can retain low resolution information including illmination.
            lapyr[i] += (mask_lapyr[level-i-1]*dest_lapyr[i])
        else:
            # Use the other levels laplacian pyramid of source image
            lapyr[i] += (mask_lapyr[level-i-1]*source_lapyr[i])

    # Step 3: convert from laplacian pyramid to image
    img = lapyr2img(lapyr)

    return img
```


```python
# Pseudo-code that computes GCF
def GCF(output, level=9):
    # Step 1: ensure that the image is single- or three-channel image. If is the latter one, convert it 
    # into gray scale.
    assert output.shape[1] in [1, 3], "Can only compute GCF for 1 or 3 channel images."
    bs, _, h, w = output.shape
    if output.shape[1] == 3:
        gray_output = output[:, 0,]*0.299 + output[:, 1,]*0.587 + output[:, 2,]*0.114
    else:
        gray_output = output

    # For each level compute the GCF score
    gray_output = gray_output.view(bs, 1, h, w)
    linear_lum = (gray_output/255).pow(2.2).view(bs, 1, h, w).double() # Normalize to [0,1] and performa gamma correction
    gcfs=[0 for i in range(level)]

    for k in range(level):
        percep_lum = 100 * linear_lum.sqrt() # compute perceptual lumination
        h, w= percep_lum.shape[2], percep_lum.shape[3]

        # very slow, need further optimization
        for i in range(h):
            for j in range(w):
                cfs = abs(percep_lum[:, 0, i, j] - percep_lum[:, 0, i+1,j] if i+1 < h else percep_lum[:, 0, i, j]) + \
                abs(percep_lum[:, 0, i, j] - percep_lum[:, 0, i-1,j] if i-1 < 0 else percep_lum[:, 0, i, j]) + \
                abs(percep_lum[:, 0, i, j] - percep_lum[:, 0, i,j+1] if j+1 > w else percep_lum[:, 0, i, j]) + \
                abs(percep_lum[:, 0, i, j] - percep_lum[:, 0, i,j-1] if j-1 < 0 else percep_lum[:, 0, i, j])
                cfs /= 4
            gcfs[k] += cfs

        gcfs[k] /= h*w
        w_i = (-0.406385*(k+1)/9 + 0.334573)*(k+1)/9 + 0.0877526
        gcfs[k] *= w_i

        if k == level-1 or h == 1 or w == 1:
            break

        linear_lum = super_pixel(linear_lum) # downsample the image

    return sum(gcfs)
```


## Observation
(1) Landmark stabilization do help release the face shaking problem. This is achieved by averaging all predicted landmarks instead of pick the one with maximum confidence score, using RetinaFace model.  
(2) Forgery result generated by SimSwap trained on 512x512 CelebHQ contains severe artifacts, e.g., in texture, and requires finetuning (However, only [unofficial Simswap training code](https://github.com/a312863063/SimSwap-train) can be found on GitHub and the finetuning makes the swapped result worse).  
(3) By comparison, newly adopted face blending method do improve the final swapped quality but slightly **degrades the identity**, which caused by the used multi-band splining that uses the two lowest level information of target image.  
(4) Multi-band splining fails when the generated face is not accurately aligned with target face in semantics or positions. For example, swapped face has a bigger nose and the target face has a smaller one and there will be a "ghost" edge in the final nose area. Besides, there will also be "ghost" teeth in the final mouth area.  

![](ghost_effect.png "Figure: Ghost effect")


## Possible directions for improvement
(1) Explore more high-resolution face swapping method.  
(2) Modify multi-band splining algorithm to mitigate ghost effects.  

## Submission
- Specified 40 Videos generated by SimSwap which was officially pretrained on 512x512 images with Multi-band splining and GCF rectification.
- Same set of videos with same method but were implemented video quality degradation on via WeChat.


# Stage3
## Score of stage2  
- ### Submission 1  

| Subjective scores (graded by human)  | realisim | mouth | quality | expression |  ID  |
| ------------------------------------ | -------- | ----- | ------- | ---------- | ---- |
| ------------------------------------ |   3.215  |	3.72  |  3.645  |     3.625  | 2.93 |

| Anti-detecion score | Overall scores | Rank |
| ----- |  ----  | ---- |
| 6.683 | 23.818 | 3rd  |

- ### Submission 2(with compression)  

| Subjective scores (graded by human)  | realisim | mouth | quality | expression |  ID  |
| ------------------------------------ | -------- | ----- | ------- | ---------- | ---- |
| ------------------------------------ |   3.445  |	3.78  |  2.73   |     3.705  | 2.89 |

| Anti-detecion score | Overall scores | Rank |
| ----- |  ----  | ---- |
| 6.823 | 23.373 |  5th |

> As can be seen in the above tables (including table in stage 1), following conclusions can be reach:  
> (1) 512x512 version of SimSwap outperforms 224x224 one.  
> (2) Compression slightly improves the realisim and anti-detetion score and has negligible effect on mouth, expression and ID scores. However, it greatly degrades the quality score.  

## Methodology
(1) New face swapping algorithm [OneShotMega](https://arxiv.org/abs/2105.04932) is adopted. Its official code can be found [here](https://github.com/zyainfal/One-Shot-Face-Swapping-on-Megapixels).  
(2) Multi-band splining algorithm is adjusted to following code. And the difference is that only the lowest two level of target face areas except nose, mouth, eyes and eyebrows are combined with swapped image. This avoids ghost effects ( on nose and teeth) appears on generated face.  
(3) Multi-band splining algorithm is also adopted when uing OneShotMega, and the algorithm only combines level 4 and level 5 face ares except nose, mouth, eyes and eyebrows (only 6 levels in total). However, this only has mild improvement on the result.  

```python
def HRFSspline(spath, dpath, mask, level=6):

    source_lapyr = laplacian_pyramid(spath, level)
    dest_lapyr = laplacian_pyramid(dpath, level)
    # mask_lapyr = gaussian_pyramid(mask, level)

    face_mask_np ,mouth_mask_np, eye_mask_np, nose_mask_np = mask
    face_mask_lapyr = gaussian_pyramid(face_mask_np, level)
    mouth_mask_lapyr = gaussian_pyramid(mouth_mask_np, level)
    eye_mask_lapyr = gaussian_pyramid(eye_mask_np, level)
    nose_mask_lapyr = gaussian_pyramid(nose_mask_np, level)
    
    lapyr = []
    for i in range(level):
        lapyr.append((1-face_mask_lapyr[level-i-1] - mouth_mask_lapyr[level-i-1] - eye_mask_lapyr[level-i-1] - nose_mask_lapyr[level-i-1])*dest_lapyr[i])
        if i <= 2:
            lapyr[i] += ((face_mask_lapyr[level-i-1])*dest_lapyr[i]) + (nose_mask_lapyr[level-i-1] + mouth_mask_lapyr[level-i-1] + eye_mask_lapyr[level-i-1])*source_lapyr[i]
        else:
            lapyr[i] += ((face_mask_lapyr[level-i-1] + mouth_mask_lapyr[level-i-1] + eye_mask_lapyr[level-i-1] + nose_mask_lapyr[level-i-1])*source_lapyr[i])

    img = lapyr2img(lapyr)

    return img
```

## Observation
(1) OneShotMega generates more realistic face images in most scenario, compared with SimSwap. However, it is easy to fail as shown below and it canot produce continuous embedding which leads to abrupt changing of illumination. Besides, some generated faces are more like animation than real-world faces. Bubble artifact exists in the swapped image and it's caused by the smaller size of swapped face compared with target face and the vague background areas are blended in.  
(2) The modified multi-band splining algorithm do mitigate the ghost artifacts.  
(3) GCF rectification fails sometimes when using OneShotMega, it turns out to make the swapped result darker when the target and swapped result has great difference in illumination as shown below.
![](GCF_failure.png "Figure: Failure case of GCF rectification")


## Submission
 - 40 specified videos generated by SimSwap 512x512 version and OneShotMega. Most videos are generated by OneShotMega but videos generated by SimSwap 512x512 version are used when it is subjectively better compared with its counterpart generated by OneShotMega
 - Compressed version via ffmpeg in which the video bitrate is set to 400k

# Conclusion of DFGC22 creation track



# References
[1] SimSwap:An Efficient Framework For High Fidelity Face Swapping  
[2] Stitch it in Time: GAN-Based Facial Editing of Real Videos  
[3] High-Resolution Neural Face Swapping for Visual Effects  
[4] One shot face swapping on megapixels  
[5] Image Quality Assessment: From Error Visibility to Structural Similarity  
[6] Multi-Scale Structural Similarity for ImageE Quality Assessment  
[7] Global contrast Factor - a New Approach to Image Contrast  
[8] DFGC 2021: A DeepFake Game Competition  
